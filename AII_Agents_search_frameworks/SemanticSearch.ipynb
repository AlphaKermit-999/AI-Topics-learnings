{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8aac0bd-3b45-4a50-9181-f65b2a5647a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Semantic Search\n",
    "\n",
    "Semantic search is an advanced information retrieval technique that focuses on understanding the meaning and context behind user queries, \n",
    "rather than just matching keywords. Unlike traditional keyword-based searches, semantic search aims to deliver more relevant results by \n",
    "comprehending the intent and nuances of the query.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Meaning Understanding**:\n",
    "   - Semantic search leverages natural language processing (NLP) techniques to interpret the meaning of words and phrases within a query.\n",
    "   - It considers synonyms, antonyms, related concepts, and the overall context to provide more accurate results.\n",
    "\n",
    "2. **Vector Representations**:\n",
    "   - Words and sentences are represented as vectors in high-dimensional spaces using models like Word2Vec, GloVe, or more advanced \n",
    "transformer-based models such as BERT, GPT, and Sentence-BERT.\n",
    "   - These vector representations capture semantic relationships between words and phrases.\n",
    "\n",
    "3. **Semantic Similarity**:\n",
    "   - Semantic search calculates the similarity between query vectors and document vectors based on their proximity in the vector space.\n",
    "   - Documents with higher similarity scores are considered more relevant to the query.\n",
    "\n",
    "4. **Contextual Understanding**:\n",
    "   - Semantic search takes into account the context in which words appear, allowing it to better understand the intent behind a query.\n",
    "   - This is particularly useful for queries that involve multiple meanings or ambiguous terms.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Improved Relevance**:\n",
    "   - Semantic search delivers more relevant results by understanding the underlying meaning of queries.\n",
    "   - It reduces the number of irrelevant documents returned, providing users with better and more accurate information.\n",
    "\n",
    "2. **Handling Synonyms and Related Terms**:\n",
    "   - Semantic search can identify synonyms and related terms, expanding the scope of a query to include semantically similar concepts.\n",
    "   - This helps in capturing a broader range of relevant content.\n",
    "\n",
    "3. **Better Understanding of Context**:\n",
    "   - By considering the context in which words appear, semantic search can interpret queries more accurately, especially those involving \n",
    "complex or nuanced language.\n",
    "\n",
    "4. **Enhanced User Experience**:\n",
    "   - Users are more likely to find what they are looking for with fewer clicks and less time spent filtering through irrelevant results.\n",
    "   - This leads to a better overall user experience on search platforms.\n",
    "\n",
    "### Applications\n",
    "\n",
    "Semantic search is widely used in various applications, including:\n",
    "\n",
    "1. **Search Engines**:\n",
    "   - Major search engines like Google use semantic understanding to provide more relevant search results.\n",
    "   \n",
    "2. **E-commerce Platforms**:\n",
    "   - Enhances product discovery by understanding customer queries and suggesting products that match their intent.\n",
    "\n",
    "3. **Content Management Systems (CMS)**:\n",
    "   - Helps in finding the most relevant content for users, improving user engagement and satisfaction.\n",
    "\n",
    "4. **Chatbots and Virtual Assistants**:\n",
    "   - Provides more accurate responses to user queries by understanding the context and meaning of the input.\n",
    "\n",
    "5. **Academic Research**:\n",
    "   - Assists researchers in finding relevant papers and articles by comprehending complex research questions.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a query for \"best restaurants in New York City\":\n",
    "\n",
    "- **Keyword-Based Search**: Might return results containing all the words \"best,\" \"restaurants,\" \"New York City.\"\n",
    "- **Semantic Search**: Understands that the user is looking for top-rated dining establishments in NYC. It might also consider synonyms \n",
    "like \"top restaurants\" or related concepts like \"high-end eateries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5780e8-a90b-438b-8f67-4d62e116a6af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.2.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.28.0-py3-none-any.whl (464 kB)\n",
      "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 7.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/9.7 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.1/9.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 5.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 4.4 MB/s eta 0:00:00\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 3.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/12.6 MB 2.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 3.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 3.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 3.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.6 MB 877.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.6 MB 877.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.6 MB 877.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.6 MB 877.5 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.6 MB 762.8 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 2.9/12.6 MB 776.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 3.1/12.6 MB 778.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 3.1/12.6 MB 778.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 3.1/12.6 MB 778.7 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 3.1/12.6 MB 778.7 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 691.8 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 691.8 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 691.8 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 677.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 677.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 677.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 677.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 677.4 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 3.9/12.6 MB 600.8 kB/s eta 0:00:15\n",
      "   ------------- -------------------------- 4.2/12.6 MB 615.4 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.6 MB 615.4 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.6 MB 615.4 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.6 MB 615.4 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 4.5/12.6 MB 568.7 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.5/12.6 MB 568.7 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.5/12.6 MB 568.7 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.7/12.6 MB 567.0 kB/s eta 0:00:14\n",
      "   -------------- ------------------------- 4.7/12.6 MB 567.0 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 5.0/12.6 MB 570.9 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 586.0 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 586.0 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 586.0 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 568.8 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 568.8 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 568.8 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 5.8/12.6 MB 561.0 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 5.8/12.6 MB 561.0 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 5.8/12.6 MB 561.0 kB/s eta 0:00:13\n",
      "   ------------------- -------------------- 6.0/12.6 MB 550.9 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 6.0/12.6 MB 550.9 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 6.3/12.6 MB 557.6 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 6.6/12.6 MB 572.0 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 600.0 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 615.5 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 615.5 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.6/12.6 MB 618.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.6 MB 625.4 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 625.4 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 627.6 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 627.6 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.6 MB 631.2 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.4/12.6 MB 631.2 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.4/12.6 MB 631.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 617.8 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 617.8 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 616.5 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 626.2 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.6 MB 649.5 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.2/12.6 MB 674.7 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 699.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 724.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.6 MB 751.0 kB/s eta 0:00:02\n",
      "   ---------------------------------------  12.6/12.6 MB 789.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 789.1 kB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, Pillow, numpy, networkx, MarkupSafe, joblib, idna, fsspec, filelock, charset-normalizer, certifi, scipy, requests, jinja2, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.17.0 fsspec-2024.12.0 huggingface-hub-0.28.0 idna-3.10 jinja2-3.1.5 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 setuptools-75.8.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.48.1 typing-extensions-4.12.2 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68a2d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from faiss-cpu) (2.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Using cached faiss_cpu-1.9.0.post1-cp312-cp312-win_amd64.whl (13.8 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b90abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.*\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17667654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.37-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.32 (from langchain)\n",
      "  Downloading langchain_core-0.3.32-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from langchain) (2.2.2)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.32->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pkumarhansda\\desktop\\ai agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-0.3.16-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 1.5 MB/s eta 0:00:00\n",
      "Using cached aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Downloading langchain_core-0.3.32-py3-none-any.whl (412 kB)\n",
      "Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.3.2-py3-none-any.whl (333 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached SQLAlchemy-2.0.37-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Using cached propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, tenacity, sniffio, pydantic-core, propcache, orjson, multidict, jsonpointer, h11, greenlet, frozenlist, attrs, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, aiosignal, httpx, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.16 langchain-core-0.3.32 langchain-text-splitters-0.3.5 langsmith-0.3.2 multidict-6.1.0 orjson-3.10.15 propcache-0.2.1 pydantic-2.10.6 pydantic-core-2.27.2 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 yarl-1.18.3 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7f93be-d17a-4041-874d-c9f9b62d21ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b0d1f9-b332-4900-b14b-9bd7e715576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, content: str, metadata: dict = None, id: int = None):\n",
    "        self.content = content\n",
    "        self.metadata = metadata\n",
    "        self.id = id\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self, documents: List[Document], model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the semantic search system with a pre-trained Sentence-BERT model.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Document]): A list of Document objects.\n",
    "            model_name (str): Name of the Sentence-BERT model to use.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.doc_vectors = None\n",
    "        self.vector_store = None\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess the input text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "            \n",
    "        Returns:\n",
    "            str: Processed text.\n",
    "        \"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"\n",
    "        Preprocess and encode documents into vectors.\n",
    "        \"\"\"\n",
    "        processed_docs = [self.preprocess_text(doc.content) for doc in self.documents]\n",
    "        self.doc_vectors = self.model.encode(processed_docs, convert_to_numpy=True)\n",
    "        \n",
    "        # Print the shape of the document vectors to verify dimensions\n",
    "        print(f\"Document vectors shape: {self.doc_vectors.shape}\")\n",
    "        \n",
    "        # Create FAISS index\n",
    "        d = self.doc_vectors.shape[1]  # Dimension of the vectors\n",
    "        self.vector_store = faiss.IndexFlatL2(d)\n",
    "        self.vector_store.add(self.doc_vectors)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query (str): Search query.\n",
    "            top_k (int): Number of top documents to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Retrieved documents.\n",
    "        \"\"\"\n",
    "        # Preprocess the query\n",
    "        processed_query = self.preprocess_text(query)\n",
    "        \n",
    "        # Encode the query\n",
    "        query_vector = self.model.encode(processed_query, convert_to_numpy=True).reshape(1, -1)\n",
    "        \n",
    "        # Print the shape of the query vector to verify dimensions\n",
    "        print(f\"Query vector shape: {query_vector.shape}\")\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        distances, indices = self.vector_store.search(query_vector, top_k)\n",
    "        \n",
    "        # Retrieve and return the most similar documents\n",
    "        retrieved_docs = [self.documents[idx].content for idx in indices[0]]\n",
    "        return retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb44dec-72c7-4634-85ba-7be87b7e7eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vectors shape: (3, 384)\n",
      "Query vector shape: (1, 384)\n",
      "The quick brown fox jumps over the lazy dog\n",
      "Quickly brown dogs never jump over fences\n",
      "Never jump over the lazy dog quickly\n",
      "Quickly brown dogs never jump over fences\n",
      "Quickly brown dogs never jump over fences\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "documents = [\n",
    "    Document(\"The quick brown fox jumps over the lazy dog\"),\n",
    "    Document(\"Never jump over the lazy dog quickly\"),\n",
    "    Document(\"Quickly brown dogs never jump over fences\")\n",
    "]\n",
    "\n",
    "semantic_search = SemanticSearch(documents)\n",
    "semantic_search.load_documents()\n",
    "\n",
    "query = \"quick brown dog\"\n",
    "top_5_docs = semantic_search.search(query, top_k=5)\n",
    "\n",
    "for doc in top_5_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7926d0ed-2916-4b03-b32a-a477b69d34b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_1 = Document(\n",
    "    content=\"\"\"The core architecture of Informatica Cloud Integration (IICS) includes a cloud infrastructure, data management layer, integration \n",
    "services layer, analytics and reporting layer, and user interface.\"\"\",\n",
    "    metadata={\"source\": \"informatica_cloud_architecture\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    content=\"\"\"Informatica Cloud differs from traditional on-premises Informatica PowerCenter in terms of deployment model, scalability, accessibility, \n",
    "and cost management.\"\"\",\n",
    "    metadata={\"source\": \"informatica_cloud_vs_powercenter\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    content=\"\"\"The various deployment models in Informatica Cloud include Public Cloud, Private Cloud, and Hybrid Cloud, each offering different levels \n",
    "of control and security.\"\"\",\n",
    "    metadata={\"source\": \"deployment_models\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    content=\"\"\"My approach to designing complex cloud data integration mappings involves modular design, data flow analysis, \n",
    "    and performance testing.\"\"\",\n",
    "    metadata={\"source\": \"integration_design\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    content=\"\"\"To optimize performance in large-scale cloud data integration projects, I utilize parallel processing, efficient resource allocation, and \n",
    "caching mechanisms.\"\"\",\n",
    "    metadata={\"source\": \"performance_optimization\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    content=\"\"\"Batch processing involves processing large volumes of data at scheduled intervals, while real-time processing enables immediate data \n",
    "processing with low latency.\"\"\",\n",
    "    metadata={\"source\": \"batch_vs_real_time\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    content=\"\"\"My experience with Intelligent Data Lake includes understanding its key features such as automated data cataloging, intelligent search, \n",
    "and advanced analytics capabilities.\"\"\",\n",
    "    metadata={\"source\": \"intelligent_data_lake\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    content=\"\"\"Handling complex data transformations using Informatica Cloud's mapping design involves utilizing complex expressions, conditional logic, \n",
    "and advanced functions.\"\"\",\n",
    "    metadata={\"source\": \"complex_transformations\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    content=\"\"\"The process of creating and managing connection configurations in IICS includes defining them using the UI or programmatically via APIs, \n",
    "and securing sensitive information using encryption.\"\"\",\n",
    "    metadata={\"source\": \"connection_configurations\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    content=\"\"\"A challenging data migration project I completed involved migrating data from on-premises systems to a cloud environment while ensuring \n",
    "minimal downtime and data integrity.\"\"\",\n",
    "    metadata={\"source\": \"data_migration_project\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "document_11 = Document(\n",
    "    content=\"\"\"To design a scalable, fault-tolerant cloud integration solution, I implemented redundancy in processing components, used load balancing \n",
    "techniques, and employed automated failover mechanisms.\"\"\",\n",
    "    metadata={\"source\": \"scalable_solution_design\"},\n",
    "    id=11,\n",
    ")\n",
    "\n",
    "document_12 = Document(\n",
    "    content=\"\"\"Strategies for handling data quality and data validation in cloud integrations include utilizing data quality tools within IICS and \n",
    "implementing error handling and retry logic.\"\"\",\n",
    "    metadata={\"source\": \"data_quality_validation\"},\n",
    "    id=12,\n",
    ")\n",
    "\n",
    "document_13 = Document(\n",
    "    content=\"\"\"The key differences between Informatica Cloud Data Integration and Application Integration involve the focus on moving and transforming \n",
    "large volumes of data versus integrating business processes and applications through APIs and messaging protocols.\"\"\",\n",
    "    metadata={\"source\": \"data_vs_application_integration\"},\n",
    "    id=13,\n",
    ")\n",
    "\n",
    "document_14 = Document(\n",
    "    content=\"\"\"My knowledge of security configurations in Informatica Cloud includes implementing role-based access control (RBAC), utilizing encryption \n",
    "for data at rest and in transit, and configuring secure connections using SSL/TLS.\"\"\",\n",
    "    metadata={\"source\": \"security_configurations\"},\n",
    "    id=14,\n",
    ")\n",
    "\n",
    "document_15 = Document(\n",
    "    content=\"\"\"My experience with pre-built connectors and custom connector development involves leveraging pre-built connectors for common systems like \n",
    "Salesforce, SAP, and Oracle, as well as developing custom connectors for unique integration requirements using Informatica’s connector SDK.\"\"\",\n",
    "    metadata={\"source\": \"connectors\"},\n",
    "    id=15,\n",
    ")\n",
    "\n",
    "document_16 = Document(\n",
    "    content=\"\"\"To diagnose and resolve performance bottlenecks in cloud integration workflows, I use profiling tools to identify slow-running tasks and \n",
    "optimize data flow paths, while increasing resource allocation where necessary.\"\"\",\n",
    "    metadata={\"source\": \"performance_bottlenecks\"},\n",
    "    id=16,\n",
    ")\n",
    "\n",
    "document_17 = Document(\n",
    "    content=\"\"\"The monitoring and logging capabilities in Informatica Cloud include utilizing the built-in monitoring dashboard to track integration \n",
    "activities and performance metrics, as well as configuring logging for detailed error tracking and auditing purposes.\"\"\",\n",
    "    metadata={\"source\": \"monitoring_logging\"},\n",
    "    id=17,\n",
    ")\n",
    "\n",
    "document_18 = Document(\n",
    "    content=\"\"\"My strategies for error handling and retry mechanisms involve implementing robust error handling logic within mappings and workflows, as \n",
    "well as setting up retry mechanisms with exponential backoff to handle transient errors gracefully.\"\"\",\n",
    "    metadata={\"source\": \"error_handling_retry\"},\n",
    "    id=18,\n",
    ")\n",
    "\n",
    "document_19 = Document(\n",
    "    content=\"\"\"My approach to implementing real-time data synchronization across multiple cloud platforms involves using real-time data streaming \n",
    "services like Apache Kafka or AWS Kinesis, as well as implementing change data capture (CDC) techniques.\"\"\",\n",
    "    metadata={\"source\": \"real_time_synchronization\"},\n",
    "    id=19,\n",
    ")\n",
    "\n",
    "document_20 = Document(\n",
    "    content=\"\"\"Ensuring data governance and compliance in cloud integration projects involves establishing data governance policies and procedures for \n",
    "managing metadata and access rights, while complying with industry regulations like GDPR, HIPAA, and CCPA through secure data handling practices.\"\"\",\n",
    "    metadata={\"source\": \"data_governance_compliance\"},\n",
    "    id=20,\n",
    ")\n",
    "\n",
    "document_21 = Document(\n",
    "    content=\"\"\"The process of creating and managing intelligent data services in Informatica Cloud includes defining them using the IICS UI or \n",
    "programmatically via APIs, as well as managing service configurations and deploying them to different environments.\"\"\",\n",
    "    metadata={\"source\": \"intelligent_data_services\"},\n",
    "    id=21,\n",
    ")\n",
    "\n",
    "document_22 = Document(\n",
    "    content=\"\"\"My experience with complex mapping transformations like lookup, router, and aggregator involves utilizing lookup transformations for \n",
    "reference data lookups, implementing router transformations to route data based on conditional logic, and using aggregators for summarizing data across \n",
    "multiple records.\"\"\",\n",
    "    metadata={\"source\": \"complex_mapping_transformations\"},\n",
    "    id=22,\n",
    ")\n",
    "\n",
    "document_23 = Document(\n",
    "    content=\"\"\"Handling incremental data loading and change data capture (CDC) involves configuring CDC settings in source systems to capture changes, \n",
    "as well as implementing incremental load strategies using timestamps or change keys in mappings.\"\"\",\n",
    "    metadata={\"source\": \"incremental_data_loading\"},\n",
    "    id=23,\n",
    ")\n",
    "\n",
    "document_24 = Document(\n",
    "    content=\"\"\"My experience integrating Informatica Cloud with major cloud platforms like AWS, Azure, and Google Cloud includes leveraging their \n",
    "respective services for seamless data integration.\"\"\",\n",
    "    metadata={\"source\": \"cloud_platform_integration\"},\n",
    "    id=24,\n",
    ")\n",
    "\n",
    "document_25 = Document(\n",
    "    content=\"\"\"Managing cloud data integration across different SaaS applications involves utilizing pre-built connectors for popular systems like \n",
    "Salesforce, ServiceNow, and MarketMuse, as well as implementing custom integrations where required using APIs and webhooks.\"\"\",\n",
    "    metadata={\"source\": \"saas_integration\"},\n",
    "    id=25,\n",
    ")\n",
    "\n",
    "document_26 = Document(\n",
    "    content=\"\"\"My proficiency with Informatica Cloud REST APIs includes using them to automate tasks such as workflow execution, asset management, and \n",
    "monitoring, while integrating external systems using API calls for real-time data exchange.\"\"\",\n",
    "    metadata={\"source\": \"rest_apis\"},\n",
    "    id=26,\n",
    ")\n",
    "\n",
    "document_27 = Document(\n",
    "    content=\"\"\"Using PowerCenter mappings in a cloud integration context involves leveraging them within IICS for complex ETL processes while ensuring \n",
    "compatibility between on-premises and cloud environments.\"\"\",\n",
    "    metadata={\"source\": \"powercenter_mappings\"},\n",
    "    id=27,\n",
    ")\n",
    "\n",
    "document_28 = Document(\n",
    "    content=\"\"\"My experience with custom scripting for complex integration scenarios includes writing scripts using Informatica’s scripting language to \n",
    "handle unique integration requirements, as well as integrating third-party tools and libraries to extend functionality.\"\"\",\n",
    "    metadata={\"source\": \"custom_scripting\"},\n",
    "    id=28,\n",
    ")\n",
    "\n",
    "document_29 = Document(\n",
    "    content=\"\"\"Designing an end-to-end data integration workflow for a complex business scenario involves identifying key data sources, targets, and \n",
    "transformation needs, as well as designing mappings and workflows that meet business objectives and performance requirements.\"\"\",\n",
    "    metadata={\"source\": \"end_to_end_workflow\"},\n",
    "    id=29,\n",
    ")\n",
    "\n",
    "document_30 = Document(\n",
    "    content=\"\"\"A project where I solved a critical data integration challenge using Informatica Cloud involved addressing issues related to real-time \n",
    "data synchronization across multiple systems, while implementing solutions using IICS features like CDC and real-time streaming services.\"\"\",\n",
    "    metadata={\"source\": \"critical_integration_challenge\"},\n",
    "    id=30,\n",
    ")\n",
    "\n",
    "document_31 = Document(\n",
    "    content=\"\"\"My internal architecture of Intelligent Cloud Services involves exploring the underlying mechanisms that support IICS, including service \n",
    "discovery, load balancing, and failover mechanisms, as well as analyzing how data is processed and managed within the cloud environment.\"\"\",\n",
    "    metadata={\"source\": \"internal_architecture\"},\n",
    "    id=31,\n",
    ")\n",
    "\n",
    "document_32 = Document(\n",
    "    content=\"\"\"Advanced techniques for managing large-scale data transformations include employing distributed processing techniques to handle large \n",
    "volumes of data efficiently, while utilizing parallel execution and caching strategies to optimize performance.\"\"\",\n",
    "    metadata={\"source\": \"large_scale_transformations\"},\n",
    "    id=32,\n",
    ")\n",
    "\n",
    "document_33 = Document(\n",
    "    content=\"\"\"Ensuring data privacy and security in multi-tenant cloud environments involves implementing isolation mechanisms to protect \n",
    "tenant-specific data, as well as using encryption, access controls, and audit logging to maintain security standards.\"\"\",\n",
    "    metadata={\"source\": \"data_privacy_security\"},\n",
    "    id=33,\n",
    ")\n",
    "\n",
    "document_34 = Document(\n",
    "    content=\"\"\"Integrating AI and machine learning capabilities with Informatica Cloud involves exploring how AI/ML can be used for predictive \n",
    "analytics, anomaly detection, and automated decision-making in integrations, while planning strategies that leverage machine learning models to enhance \n",
    "data processing.\"\"\",\n",
    "    metadata={\"source\": \"ai_ml_integration\"},\n",
    "    id=34,\n",
    ")\n",
    "\n",
    "document_35 = Document(\n",
    "    content=\"\"\"The future of cloud data integration is expected to evolve with the continued growth of serverless architectures and event-driven \n",
    "integration patterns, as well as increased adoption of AI/ML for automation and intelligence in cloud integrations.\"\"\",\n",
    "    metadata={\"source\": \"future_of_cloud_integration\"},\n",
    "    id=35,\n",
    ")\n",
    "\n",
    "document_36 = Document(\n",
    "    content=\"\"\"\n",
    "### Chapter 1: Taskflows and Linear Taskflows\n",
    "\n",
    "#### Section 2.1: Taskflows\n",
    "\n",
    "1. **Taskflow Steps**\n",
    "   - The different types of taskflow steps available in Informatica IICS include Data Task, Notification Task, Decision Step, File Watch Task, etc.\n",
    "\n",
    "2. **Creating a Taskflow**\n",
    "   - To create a new taskflow in Informatica IICS, you typically start by selecting the appropriate template (e.g., Single Task, Sequential Tasks with \n",
    "Decision) and then configure the steps and properties as needed.\n",
    "\n",
    "3. **Taskflow Templates**\n",
    "   - The types of taskflow templates available include Single Task for running one task on a schedule, and Sequential Tasks with Decision for running tasks \n",
    "in sequence followed by a decision step based on output. Each template is suited to different automation needs.\n",
    "\n",
    "4. **Setting Taskflow Properties**\n",
    "   - Important properties that can be set for a taskflow include the name, location, input fields, output fields, temporary fields, advanced properties, \n",
    "and notes. These are configured through the Properties section in the taskflow designer.\n",
    "\n",
    "5. **Setting Taskflow Step Properties**\n",
    "   - For various taskflow steps like Data Task, Notification Task, and Decision step, you set properties by accessing the Properties section for each step \n",
    "within the taskflow designer.\n",
    "\n",
    "6. **Runtime Parameters**\n",
    "   - Runtime parameters are variables that can be overridden at runtime to customize task behavior without changing the underlying code. They are used to \n",
    "make tasks more flexible and adaptable to different scenarios.\n",
    "\n",
    "7. **Parameter Set**\n",
    "   - A parameter set is a collection of parameters that can be reused across multiple tasks or steps within a taskflow, promoting consistency and reducing \n",
    "redundancy.\n",
    "\n",
    "8. **The Expression Editor**\n",
    "   - The Expression Editor is used to create expressions within a taskflow for conditional logic, calculations, or data transformations. It allows users to \n",
    "define complex operations using drag-and-drop components and scripting.\n",
    "\n",
    "9. **Taskflow Functions**\n",
    "   - Common taskflow functions include IF-THEN-ELSE, FOR-EACH, WHILE, etc., which are used to control the flow of execution based on specific conditions or \n",
    "loops.\n",
    "\n",
    "10. **Running a Taskflow**\n",
    "    - Methods to run a taskflow include manually from the taskflow designer, using APIs like REST, or through scheduled jobs. The RunAJob utility can also \n",
    "be used to execute taskflows with specified inputs.\n",
    "\n",
    "11. **Taskflow Example**\n",
    "    - A simple taskflow example might involve a Data Task step to extract data from a source system, followed by a Notification Task step to send an email \n",
    "if the extraction fails, and finally a Decision Step to determine whether to proceed with further processing based on the success or failure of the initial \n",
    "steps.\n",
    "\n",
    "#### Section 2.1.12: Running a Taskflow\n",
    "\n",
    "1. **Running a Taskflow from the Taskflow Designer**\n",
    "   - To run a taskflow directly from the taskflow designer, you typically click the Run button within the designer interface.\n",
    "\n",
    "2. **Using Taskflow Inputs**\n",
    "   - Taskflow inputs are created and used by adding input fields to the taskflow and configuring them with values or expressions that will be passed to the \n",
    "tasks when executed.\n",
    "\n",
    "3. **Publishing a Taskflow**\n",
    "   - Publishing a taskflow involves saving it in a state where it can be accessed and run by other users or systems. It is important for making changes \n",
    "available and ensuring consistency across different environments.\n",
    "\n",
    "4. **Running a Taskflow as an API**\n",
    "   - To run a taskflow using REST APIs, you make HTTP requests to the appropriate endpoints, passing inputs as required, and handle responses to monitor \n",
    "execution status and results.\n",
    "\n",
    "5. **Adding a Custom Name to a Taskflow Name**\n",
    "   - A custom name can be added to a taskflow when running it by specifying an override API name in the taskflow properties or through the RunAJob utility.\n",
    "\n",
    "6. **Scheduling a Taskflow**\n",
    "   - Scheduling a taskflow involves creating a schedule and associating it with the desired taskflow. It is important for automating repetitive tasks at \n",
    "specific times or intervals.\n",
    "\n",
    "7. **Monitoring Taskflow Status**\n",
    "   - The status of a running taskflow can be monitored using APIs that provide real-time updates on execution progress, success/failure outcomes, and other \n",
    "relevant metrics.\n",
    "\n",
    "#### Section 2.1.15: Taskflow Log Files\n",
    "\n",
    "1. **Downloading Taskflow Log File**\n",
    "   - To download a taskflow log file from Data Integration, you navigate to the My Jobs page, select the taskflow job, and use the provided options to \n",
    "access and download the log resource.\n",
    "\n",
    "2. **Taskflow Log File Contents**\n",
    "   - A taskflow log file contains information such as the asset name, type, duration, start/end times, location, run ID, URLs, runtime environment, status, \n",
    "subtask details, and error messages. This information is crucial for troubleshooting and auditing purposes.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"chapter1\"},\n",
    "    id=36,\n",
    ")\n",
    "\n",
    "document_37 = Document(\n",
    "    content=\"\"\"\n",
    "### Chapter 3: Linear Taskflows\n",
    "\n",
    "#### Section 3.1: Scheduling Linear Taskflow Jobs\n",
    "\n",
    "1. **Configuring a Linear Taskflow**\n",
    "   - Configuring a linear taskflow involves adding tasks in sequence, setting their order using sequence numbers, configuring error handling options like \n",
    "Stop on Error, and optionally setting email notification preferences.\n",
    "\n",
    "2. **Running a Linear Taskflow**\n",
    "   - Steps to run a scheduled linear taskflow include navigating to the Explore page, selecting the taskflow, clicking Actions, and choosing Run. \n",
    "Alternatively, you can configure a schedule for the taskflow to run automatically at specified intervals.\n",
    "\n",
    "3. **Stopping a Linear Taskflow or Subtask**\n",
    "   - To stop a running linear taskflow or its subtasks, you navigate to the My Jobs page, select the job, and use the Stop button. The behavior upon \n",
    "stopping depends on whether the Stop on Error option is enabled for the taskflow.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"chapter3\"},\n",
    "    id=37,\n",
    ")\n",
    "document_38 = Document(\n",
    "    content=\"\"\"Question: What is Informatica Cloud and what are its capabilities?\n",
    "Answer: Informatica Cloud is a comprehensive platform for data integration and business intelligence. Its capabilities include ETL \n",
    "(Extract, Transform, Load) services, data replication, analytics tools, and more. It supports various connectors to integrate with \n",
    "different data sources, perform data transformations, and manage data pipelines efficiently.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=38,\n",
    ")\n",
    "\n",
    "document_39 = Document(\n",
    "    content=\"\"\"Question: What is ETL in the context of Informatica Cloud Data Integration?\n",
    "Answer: In Informatica Cloud, ETL stands for Extract, Transform, and Load. It refers to the process of extracting data from source \n",
    "systems, transforming it into a desired format, and loading it into target systems or data warehouses. Informatica Cloud provides tools \n",
    "and services to automate this process, ensuring data accuracy and consistency across different environments.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=39,\n",
    ")\n",
    "\n",
    "document_40 = Document(\n",
    "    content=\"\"\"Question: Explain ETL concepts in detail with respect to Informatica Cloud.\n",
    "Answer: ETL is the process of extracting data from various sources, transforming it into a usable format, and loading it into a data \n",
    "warehouse or another target system. In Informatica Cloud, this involves several key steps:\n",
    "1. **Extract**: Gathering data from sources using extractors.\n",
    "2. **Transform**: Cleaning, enriching, and formatting data to meet business requirements using transformation mappings.\n",
    "3. **Load**: Storing the transformed data in the target systems using loaders. Informatica Cloud supports a wide range of connectors for \n",
    "seamless integration with different data sources and targets.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=40,\n",
    ")\n",
    "\n",
    "document_41 = Document(\n",
    "    content=\"\"\"Question: Explain Data Warehousing.\n",
    "Answer: Data warehousing is the process of consolidating, organizing, and storing large amounts of historical data for analysis. It \n",
    "involves creating a centralized repository where data from multiple sources is integrated and made available for reporting, analytics, \n",
    "and decision-making. Data warehouses are designed to support complex queries and provide insights that drive business strategies.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"database\"},\n",
    "    id=41,\n",
    ")\n",
    "\n",
    "document_42 = Document(\n",
    "    content=\"\"\"Question: Explain Dimensional Modeling in detail.\n",
    "Answer: Dimensional modeling is a data modeling approach used in data warehouses. It involves creating dimension tables and fact tables \n",
    "to represent business concepts and relationships. Dimensions contain descriptive attributes, while facts store the actual measured \n",
    "values.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"database\"},\n",
    "    id=42,\n",
    ")\n",
    "\n",
    "document_43 = Document(\n",
    "    content=\"\"\"Question: What are the different types of SCD (Slowly Changing Dimension) in Informatica Cloud?\n",
    "Answer: SCD is a technique used to handle changes in dimension data. There are four main types:\n",
    "1. **Type 0 SCD**: Replace entire records with new ones.\n",
    "2. **Type 1 SCD**: Overwrite old records with new ones, keeping only the latest values.\n",
    "3. **Type 2 SCD**: Keep a history of all changes by adding new records without deleting old ones.\n",
    "4. **Type 3 SCD**: Track which version is current and when it was valid, typically using a status column.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=43,\n",
    ")\n",
    "\n",
    "document_44 = Document(\n",
    "    content=\"\"\"Question: How do you load Reference Tables and Hierarchies in Informatica Cloud?\n",
    "Answer: Reference tables and hierarchies can be loaded similarly to dimensions, using one-time or incremental loads. For example:\n",
    "- **Customer Dim**: Loading customer details with attributes like name, address, etc.\n",
    "- **Time Dim**: Creating a comprehensive time dimension for date-related queries.\n",
    "- **Region Dim**: Defining regions with hierarchies and attributes like region manager, country, etc.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=44,\n",
    ")\n",
    "\n",
    "document_45 = Document(\n",
    "    content=\"\"\"Question: What are fact tables in Informatica Cloud?\n",
    "Answer: Fact tables are the core of a data warehouse. They contain the measurable values or metrics about business processes and events. \n",
    "Each fact table is linked to dimension tables, providing context and attributes for the facts. In Informatica Cloud, fact tables store \n",
    "large volumes of transactional data that can be analyzed using OLAP (Online Analytical Processing) tools.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"database\"},\n",
    "    id=45,\n",
    ")\n",
    "\n",
    "document_46 = Document(\n",
    "    content=\"\"\"Question: What are the different load strategies for Fact Tables in Informatica Cloud?\n",
    "Answer: There are several strategies to load fact tables in Informatica Cloud:\n",
    "1. **Full Load**: Replacing all existing data with new data.\n",
    "2. **Incremental Load**: Adding only new or changed records without deleting old ones.\n",
    "3. **Type 0 SCD (Slowly Changing Dimension)**: Similar to Type 1, but typically used for fact tables by appending changes as new rows \n",
    "rather than overwriting.\n",
    "4. **Type 1 SCD**: Overwriting existing data with new values in a fact table.\n",
    "5. **Type 2 SCD**: Maintaining a history of all changes in a fact table, similar to dimension tables.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=46,\n",
    ")\n",
    "\n",
    "document_47 = Document(\n",
    "    content=\"\"\"Question: Explain in a step by step approach how to build a mapping in Informatica Cloud to implement a TYPE1 \n",
    "dimension (Customer Dimension).\n",
    "Answer: To create a TYPE1 dimension for customers:\n",
    "1. **Create the Customer Dimension Table**: Design and create a table with attributes like customer_id, name, address, etc.\n",
    "2. **Prepare Source Data**: Ensure your source data is clean and ready for loading into the dimension table.\n",
    "3. **Build the Mapping**:\n",
    "   - Open Informatica Cloud Studio.\n",
    "   - Create a new mapping for the Customer Dimension.\n",
    "4. **Configure Source and Target**:\n",
    "   - Set up the source connector (e.g., Oracle, SQL Server).\n",
    "   - Configure the target as your customer dimension table.\n",
    "5. **Add Transformation Logic**: \n",
    "   - Use the Expression transformation to handle any necessary transformations.\n",
    "6. **Load Data**: \n",
    "   - Apply a full load strategy using the Update Strategy set to \"INSERT_ONLY\".\n",
    "7. **Test and Validate**: \n",
    "   - Execute the mapping and validate that all data is loaded correctly into the dimension table.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=47,\n",
    ")\n",
    "\n",
    "document_48 = Document(\n",
    "    content=\"\"\"Question: Explain in a step by step approach how to build a mapping in Informatica Cloud to implement a TYPE2 \n",
    "dimension (Customer Dimension).\n",
    "Answer: To create a TYPE2 dimension for customers:\n",
    "1. **Create the Customer Dimension Table**: Design and create a table with attributes like customer_id, name, address, start_date, \n",
    "end_date, etc.\n",
    "2. **Prepare Source Data**: Ensure your source data is clean and ready for loading into the dimension table.\n",
    "3. **Build the Mapping**:\n",
    "   - Open Informatica Cloud Studio.\n",
    "   - Create a new mapping for the Customer Dimension.\n",
    "4. **Configure Source and Target**:\n",
    "   - Set up the source connector (e.g., Oracle, SQL Server).\n",
    "   - Configure the target as your customer dimension table.\n",
    "5. **Add Transformation Logic**: \n",
    "   - Use the Expression transformation to handle any necessary transformations.\n",
    "6. **Load Data**: \n",
    "   - Apply a full load strategy using the Update Strategy set to \"INSERT_ONLY\".\n",
    "7. **Maintain History**: \n",
    "   - In your mapping, add logic in the target insert or update to manage start_date and end_date for historical records.\n",
    "8. **Test and Validate**: \n",
    "   - Execute the mapping and validate that all data is loaded correctly into the dimension table with history tracking.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=48,\n",
    ")\n",
    "\n",
    "document_49 = Document(\n",
    "    content=\"\"\"Question: Explain in a step by step approach how to build a mapping in Informatica Cloud to implement a TYPE1 fact \n",
    "(Invoice Line Fact).\n",
    "Answer: To create a TYPE1 fact for invoice lines:\n",
    "1. **Create the Invoice Line Fact Table**: Design and create a table with attributes like invoice_id, line_number, product_id, quantity, \n",
    "amount, etc.\n",
    "2. **Prepare Source Data**: Ensure your source data is clean and ready for loading into the fact table.\n",
    "3. **Build the Mapping**:\n",
    "   - Open Informatica Cloud Studio.\n",
    "   - Create a new mapping for the Invoice Line Fact.\n",
    "4. **Configure Source and Target**:\n",
    "   - Set up the source connector (e.g., Oracle, SQL Server).\n",
    "   - Configure the target as your invoice line fact table.\n",
    "5. **Add Transformation Logic**: \n",
    "   - Use the Expression transformation to handle any necessary transformations.\n",
    "6. **Load Data**: \n",
    "   - Apply a full load strategy using the Update Strategy set to \"INSERT_ONLY\".\n",
    "7. **Test and Validate**: \n",
    "   - Execute the mapping and validate that all data is loaded correctly into the fact table.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=49,\n",
    ")\n",
    "\n",
    "document_50 = Document(\n",
    "    content=\"\"\"Question: Explain in detail the Lookup transformation with atleast 5 examples.\n",
    "Answer: The Lookup transformation retrieves data from a lookup source based on the current row's values. Here are five examples:\n",
    "1. **Simple Lookup**: Retrieve customer details from a customer dimension table using a customer_id.\n",
    "2. **Dynamic Lookup**: Use dynamic SQL to fetch product prices based on product IDs and dates.\n",
    "3. **Multi-Valued Lookup**: Fetch multiple addresses for customers based on their IDs.\n",
    "4. **Hierarchical Lookup**: Retrieve product categories and subcategories from a hierarchical structure.\n",
    "5. **Conditional Lookup**: Apply conditions to filter data before lookup, e.g., only fetch active customers.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=50,\n",
    ")\n",
    "\n",
    "document_51 = Document(\n",
    "    content=\"\"\"Question: Explain in detail the Joiner transformation with atleast 5 examples.\n",
    "Answer: The Joiner transformation combines data from two or more sources based on matching keys. Here are five examples:\n",
    "1. **Basic Join**: Combine customer and order details to create a single view of sales.\n",
    "2. **Multi-way Join**: Join three tables (e.g., customers, orders, products) to create a comprehensive sales report.\n",
    "3. **Outer Joins**: Use Left or Right Outer Joins to include all records from one source even if there is no match in another.\n",
    "4. **Self-Join**: Combine data within the same table based on different conditions.\n",
    "5. **Dynamic Join**: Apply conditions dynamically to join sources, e.g., join only for recent orders.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=51,\n",
    ")\n",
    "\n",
    "document_52 = Document(\n",
    "    content=\"\"\"Question: Explain in detail the Router transformation with atleast 5 examples.\n",
    "Answer: The Router transformation routes data based on specific conditions to different target tables or processes. Here are five \n",
    "examples:\n",
    "1. **Conditional Routing**: Route sales data based on regions (e.g., East, West) to separate tables.\n",
    "2. **Priority-based Routing**: Prioritize data routing based on urgency levels (e.g., high priority orders first).\n",
    "3. **Filtering and Routing**: Filter data before routing it to specific targets, e.g., route only active customers.\n",
    "4. **Error Handling Routing**: Route error records to a dedicated table for further investigation.\n",
    "5. **Dynamic Routing**: Apply dynamic conditions at runtime based on current state or external inputs.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=52,\n",
    ")\n",
    "\n",
    "document_53 = Document(\n",
    "    content=\"\"\"Question: Explain in detail Parameters with atleast 5 examples.\n",
    "Answer: Parameters allow you to pass values between mappings and taskflows dynamically. Here are five examples:\n",
    "1. **Dynamic SQL**: Use parameters in SQL queries to filter data based on runtime values.\n",
    "2. **Conditional Logic**: Apply conditional logic based on parameter values to alter mapping behavior.\n",
    "3. **Taskflow Execution**: Pass parameters from one taskflow to another for seamless execution.\n",
    "4. **Parameterized Taskflows**: Create reusable taskflows by passing inputs and outputs through parameters.\n",
    "5. **Dynamic Update Strategies**: Adjust update strategies in mappings dynamically based on parameter values.\"\"\"\n",
    "    ,\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=53,\n",
    ")\n",
    "\n",
    "document_54 = Document(\n",
    "    content=\"\"\"\n",
    "Scenario: You have an ETL process that is running well but needs to be optimized further to reduce execution time.\n",
    "Problem: How would you set up performance monitoring and tuning in Informatica?\n",
    "Response: \n",
    "1. **Performance Metrics:** Define key performance indicators (KPIs) such as data load times, error rates, and resource utilization.\n",
    "2. **Monitoring Tools:** Utilize Informatica’s built-in monitoring tools to track performance metrics in real-time.\n",
    "3. **Tuning Strategies:** Identify bottlenecks based on monitoring data and apply tuning strategies (e.g., parallelism, partitioning).\n",
    "4. **Performance Reports:** Generate regular reports to measure the effectiveness of performance improvements.\"\"\",\n",
    "    metadata={\"source\": \"informatica\"},\n",
    "    id=54,\n",
    ")\n",
    "\n",
    "document_55 = Document(\n",
    "    content=\"\"\"\n",
    "Scenario: Your company is implementing a new ETL solution that involves handling sensitive customer data. You need to ensure compliance \n",
    "with relevant regulations (e.g., GDPR).\n",
    "Problem: How would you address data security and compliance requirements in your ETL processes?\n",
    "Response:\n",
    "1. **Data Masking:** Implement data masking to protect sensitive information during development and testing.\n",
    "2. **Encryption:** Use encryption for secure data storage and transmission.\n",
    "3. **Access Controls:** Define strict access controls based on user roles and responsibilities.\n",
    "4. **Audit Trails:** Maintain audit trails for all data operations, including ETL processes.\n",
    "5. **Compliance Checks:** Regularly perform compliance checks and audits to ensure adherence to regulatory requirements.\"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=55,\n",
    ")\n",
    "\n",
    "document_56 = Document(\n",
    "    content=\"\"\"\n",
    "Scenario: Your company is planning an ETL process that will handle a large volume of customer transaction data (e.g., billions of \n",
    "records).\n",
    "Problem: How would you approach handling such large volumes of data efficiently?\n",
    "Response:\n",
    "1. **Partitioning:** Partition data at the source system to reduce the amount of data processed during each run.\n",
    "2. **Parallelism:** Increase parallel processing capabilities to handle the large volume of data more efficiently.\n",
    "3. **Batch Processing:** Break the ETL process into smaller, manageable batches to avoid overwhelming the system.\n",
    "4. **Resource Management:** Ensure that sufficient resources (e.g., memory, CPU) are allocated for handling large volumes of data.\n",
    "5. **Performance Tuning:** Optimize mapping and workflow configurations to improve performance.\"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=56,\n",
    ")\n",
    "\n",
    "document_57 = Document(\n",
    "    content=\"\"\"\n",
    "Scenario: You need to ensure the quality of the data being loaded into your ETL process, particularly in terms of completeness and \n",
    "accuracy.\n",
    "Problem: How would you implement robust data quality checks in your Informatica mappings?\n",
    "Response:\n",
    "1. **Data Profiling:** Use Informatica’s Data Quality tools to profile data and identify missing values, duplicates, and anomalies.\n",
    "2. **Validation Rules:** Define validation rules within ETL mappings to enforce business logic (e.g., date ranges, numeric constraints).\n",
    "3. **Error Handling:** Implement error handling mechanisms to capture and log invalid records for further review.\n",
    "4. **Data Cleansing:** Use Informatica’s data cleansing functions to correct or remove invalid data before loading.\n",
    "5. **Quality Reporting:** Generate reports summarizing the results of data quality checks for stakeholders.\"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=57,\n",
    ")\n",
    "\n",
    "document_58 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 1: ETL Design for Customer Database\n",
    "\n",
    "**Scenario:** You are tasked with designing an ETL process for a customer database that includes customer details, transactions, and \n",
    "products. The system needs to handle updates frequently and ensure data consistency across different systems.\n",
    "\n",
    "**Problem:** How would you design the ETL architecture for this scenario?\n",
    "\n",
    "**Response:**\n",
    "1. **Source Definition:**\n",
    "   - Identify and configure all source systems (e.g., Customer Database, Transaction Logs, Product Catalog).\n",
    "2. **Dimension Tables Design:**\n",
    "   - **Customer Dimension:** Create a TYPE2 dimension to handle historical data.\n",
    "     - Steps:\n",
    "       1. Define the schema for the customer dimension table.\n",
    "       2. Implement a surrogate key for each customer record.\n",
    "       3. Include columns for customer attributes (e.g., name, address, phone number).\n",
    "       4. Add start and end dates to track changes over time.\n",
    "   - **Product Dimension:** Similarly, create a TYPE2 dimension for product details with historical versioning.\n",
    "     - Steps:\n",
    "       1. Define the schema for the product dimension table.\n",
    "       2. Implement a surrogate key for each product record.\n",
    "       3. Include columns for product attributes (e.g., name, category, price).\n",
    "       4. Add start and end dates to track changes over time.\n",
    "3. **Fact Table Design:**\n",
    "   - **Transaction Fact:** Design a fact table capturing transaction details, which will be linked to the customer and product \n",
    "dimensions.\n",
    "     - Steps:\n",
    "       1. Define the schema for the transaction fact table.\n",
    "       2. Include foreign keys linking to the customer and product dimension tables.\n",
    "       3. Add columns for transaction attributes (e.g., date, amount, quantity).\n",
    "4. **ETL Process Flow:**\n",
    "   - Extract data from sources using Informatica PowerCenter or Informatica Cloud Data Integration.\n",
    "     - Steps:\n",
    "       1. Set up source connectors for each system.\n",
    "       2. Define extraction queries to fetch relevant data.\n",
    "   - Transform data to match schema requirements of dimension tables (e.g., normalization, type conversion).\n",
    "     - Steps:\n",
    "       1. Use transformation components to clean and standardize data.\n",
    "       2. Apply necessary transformations (e.g., date formatting, data type conversions).\n",
    "   - Load transformed data into appropriate dimension and fact tables using TYPE2 incremental loads.\n",
    "     - Steps:\n",
    "       1. Configure the load process to handle historical changes.\n",
    "       2. Update existing records with new information and mark old records as inactive.\n",
    "5. **Error Handling and Logging:**\n",
    "   - Implement robust error handling mechanisms, logging errors, and notifications for corrective actions.\n",
    "     - Steps:\n",
    "       1. Set up error handling components in mappings.\n",
    "       2. Configure logging to capture detailed error messages.\n",
    "       3. Send notifications (e.g., email alerts) when errors occur.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=58,\n",
    ")\n",
    "\n",
    "document_59 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 2: Build and Test ETL Process\n",
    "\n",
    "**Scenario:** You need to build an ETL process in Informatica PowerCenter to load data into a data warehouse from various sources. The \n",
    "task includes loading customer details, sales transactions, and product information.\n",
    "\n",
    "**Problem:** How would you structure your Informatica project and execute the ETL process?\n",
    "\n",
    "**Response:**\n",
    "1. **Project Setup:**\n",
    "   - Create an Informatica repository and set up a new project.\n",
    "     - Steps:\n",
    "       1. Log in to Informatica PowerCenter.\n",
    "       2. Create a new repository if one does not exist.\n",
    "       3. Set up a new project within the repository.\n",
    "   - Define source systems, target tables, and dimensions in the repository.\n",
    "     - Steps:\n",
    "       1. Add source connectors for each data source (e.g., databases, files).\n",
    "       2. Define target tables in the data warehouse.\n",
    "       3. Create dimension tables as required.\n",
    "2. **Mapping Design:**\n",
    "   - Design mappings for extracting data from each source system (e.g., Customer Database, Transaction Logs).\n",
    "     - Steps:\n",
    "       1. Open the Informatica Designer and create a new mapping.\n",
    "       2. Connect source connectors to target tables.\n",
    "       3. Define transformation logic as needed.\n",
    "   - Apply transformations as necessary (e.g., normalization, type conversion).\n",
    "     - Steps:\n",
    "       1. Use transformation components like Expression Editor, Filter, Sorter, etc.\n",
    "       2. Implement data cleansing and validation rules.\n",
    "3. **Workflow Creation:**\n",
    "   - Create a workflow to orchestrate the execution of multiple mappings.\n",
    "     - Steps:\n",
    "       1. Open Informatica Workflow Manager.\n",
    "       2. Create a new workflow.\n",
    "       3. Add tasks for each mapping.\n",
    "   - Set dependencies and synchronization points to ensure proper order of operations.\n",
    "     - Steps:\n",
    "       1. Define task dependencies based on data flow.\n",
    "       2. Use control objects like Decision, Sequence, etc., to manage execution order.\n",
    "4. **Testing Strategy:**\n",
    "   - Develop unit tests for each mapping to validate data extraction and transformation logic.\n",
    "     - Steps:\n",
    "       1. Use Informatica’s Test & Trace feature.\n",
    "       2. Run test cases with sample data.\n",
    "   - Perform integration testing by running the entire workflow with sample data.\n",
    "     - Steps:\n",
    "       1. Execute the workflow with a subset of production data.\n",
    "       2. Verify that all mappings run successfully and produce expected results.\n",
    "5. **Validation and Deployment:**\n",
    "   - Validate the ETL process using a subset of production data.\n",
    "     - Steps:\n",
    "       1. Test the entire workflow end-to-end.\n",
    "       2. Ensure data integrity and consistency across target tables.\n",
    "   - Deploy the validated workflow to production and monitor its performance.\n",
    "     - Steps:\n",
    "       1. Schedule the workflow in Informatica PowerCenter.\n",
    "       2. Monitor execution logs for any issues.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=59,\n",
    ")\n",
    "\n",
    "document_60 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 3: Troubleshoot Problems in Production\n",
    "\n",
    "**Scenario:** You are monitoring your ETL processes and notice that the customer transaction load is significantly delayed. The logs \n",
    "indicate errors related to data quality issues.\n",
    "\n",
    "**Problem:** What steps would you take to diagnose and resolve the issue?\n",
    "\n",
    "**Response:**\n",
    "1. **Review Logs:**\n",
    "   - Analyze recent logs for specific error messages indicating data quality issues.\n",
    "     - Steps:\n",
    "       1. Access Informatica PowerCenter’s log files.\n",
    "       2. Search for error codes and messages related to customer transactions.\n",
    "2. **Data Profiling:**\n",
    "   - Use Informatica Data Quality tools to profile customer and transaction data, identifying discrepancies or anomalies.\n",
    "     - Steps:\n",
    "       1. Open Informatica Data Quality tool.\n",
    "       2. Load the relevant datasets (customer details, transactions).\n",
    "       3. Run profiling tasks to identify issues like missing values, duplicates, etc.\n",
    "3. **Re-run with Validation:**\n",
    "   - Rerun the ETL process with enhanced validation logic to filter out invalid records.\n",
    "     - Steps:\n",
    "       1. Modify mappings to include additional validation rules.\n",
    "       2. Execute the workflow again and monitor for errors.\n",
    "4. **Correct Source Data:**\n",
    "   - If the issue is due to incorrect source data, work with the source systems team to correct or update the data.\n",
    "     - Steps:\n",
    "       1. Identify the root cause of the data quality issues.\n",
    "       2. Communicate with the source system team for corrections.\n",
    "5. **Improve Error Handling:**\n",
    "   - Enhance error handling in mappings and workflows to prevent similar issues in future runs.\n",
    "     - Steps:\n",
    "       1. Implement more robust error handling mechanisms.\n",
    "       2. Configure notifications for recurring errors.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=60,\n",
    ")\n",
    "\n",
    "document_61 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 4: Performance Optimization Techniques\n",
    "\n",
    "**Scenario:** Your ETL process for loading transaction data from a large database is experiencing performance bottlenecks. The current \n",
    "solution is taking over an hour, and you need to optimize it.\n",
    "\n",
    "**Problem:** How would you approach optimizing the performance of this ETL process?\n",
    "\n",
    "**Response:**\n",
    "1. **Partitioning:**\n",
    "   - Identify key partition columns in source tables (e.g., date) and configure Informatica to extract only relevant partitions.\n",
    "     - Steps:\n",
    "       1. Analyze source data for suitable partition columns.\n",
    "       2. Configure the Extract stage in mappings to use partitioning.\n",
    "2. **Parallelism:**\n",
    "   - Increase parallelism by adding more sessions or distributing data across multiple physical nodes.\n",
    "     - Steps:\n",
    "       1. Adjust the number of sessions in the workflow.\n",
    "       2. Use distributed processing if available.\n",
    "3. **Index Management:**\n",
    "   - Ensure that appropriate indexes are created on source and target tables for faster query execution.\n",
    "     - Steps:\n",
    "       1. Create indexes on key columns in source databases.\n",
    "       2. Optimize indexes on target tables to improve load performance.\n",
    "4. **Incremental Loads:**\n",
    "   - Use incremental loads (e.g., TYPE1, TYPE2) to avoid full table scans during each run.\n",
    "     - Steps:\n",
    "       1. Configure mappings to extract only changed data.\n",
    "       2. Update dimension and fact tables accordingly.\n",
    "5. **Profiling and Tuning:**\n",
    "   - Utilize Informatica’s profiling tools to identify bottlenecks and make necessary tuning adjustments.\n",
    "     - Steps:\n",
    "       1. Use Informatica Profiler to analyze performance metrics.\n",
    "       2. Adjust configurations based on profiling results.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=61,\n",
    ")\n",
    "\n",
    "document_62 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 5: Best Practices in ETL Design\n",
    "\n",
    "**Scenario:** You are designing an ETL solution for a company with multiple departments, including finance, operations, and marketing. \n",
    "Each department has different data requirements and constraints.\n",
    "\n",
    "**Problem:** What best practices would you follow to ensure the success of this multi-departmental ETL project?\n",
    "\n",
    "**Response:**\n",
    "1. **Standardization:**\n",
    "   - Establish a common schema and data dictionary across all departments.\n",
    "     - Steps:\n",
    "       1. Define a standardized set of tables and columns.\n",
    "       2. Document the schema for reference.\n",
    "2. **Reusability:**\n",
    "   - Design reusable components (e.g., libraries, transformation templates) to promote consistency.\n",
    "     - Steps:\n",
    "       1. Create reusable mappings and workflows.\n",
    "       2. Use parameterization where applicable.\n",
    "3. **Security:**\n",
    "   - Ensure that access controls are in place for different departments based on their requirements.\n",
    "     - Steps:\n",
    "       1. Implement role-based access control.\n",
    "       2. Restrict data access to authorized users only.\n",
    "4. **Documentation:**\n",
    "   - Document the ETL architecture, mappings, and workflows comprehensively.\n",
    "     - Steps:\n",
    "       1. Maintain detailed documentation for each component.\n",
    "       2. Use diagrams and flowcharts where necessary.\n",
    "5. **Governance:**\n",
    "   - Establish a data governance framework to ensure data quality, lineage, and compliance.\n",
    "     - Steps:\n",
    "       1. Define policies and procedures for data management.\n",
    "       2. Monitor and audit data usage regularly.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=62,\n",
    ")\n",
    "\n",
    "document_63 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 6: Implementing Informatica MDM\n",
    "\n",
    "**Scenario:** Your company is looking to implement an Informatica MDM solution for managing customer records across multiple systems.\n",
    "\n",
    "**Problem:** How would you approach the implementation of an Informatica MDM system?\n",
    "\n",
    "**Response:**\n",
    "1. **Business Requirements Analysis:**\n",
    "   - Gather requirements from business stakeholders, including data sources, master record definition, and governance rules.\n",
    "     - Steps:\n",
    "       1. Conduct meetings with departments to understand their needs.\n",
    "       2. Document all functional and non-functional requirements.\n",
    "2. **MDM Solution Design:**\n",
    "   - Design a logical data model for master records, incorporating attributes relevant to customer management (e.g., identity, \n",
    "demographics).\n",
    "     - Steps:\n",
    "       1. Define the schema for master records.\n",
    "       2. Identify key attributes to be included in each entity.\n",
    "3. **ETL Mapping Development:**\n",
    "   - Develop ETL mappings to extract, transform, and load data from source systems into the MDM repository.\n",
    "     - Steps:\n",
    "       1. Set up source connectors for each system.\n",
    "       2. Create mappings to clean and standardize data.\n",
    "       3. Load data into the MDM hub.\n",
    "4. **Data Quality Management:**\n",
    "   - Implement Informatica Data Quality tools to cleanse and standardize data in the MDM environment.\n",
    "     - Steps:\n",
    "       1. Use Data Quality components to identify and correct errors.\n",
    "       2. Apply validation rules to ensure data integrity.\n",
    "5. **User Training and Support:**\n",
    "   - Provide training for end-users on how to use the MDM solution effectively.\n",
    "     - Steps:\n",
    "       1. Develop training materials and conduct sessions.\n",
    "       2. Offer ongoing support and assistance.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=63,\n",
    ")\n",
    "\n",
    "document_64 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 7: Performance Monitoring and Tuning\n",
    "\n",
    "**Scenario:** You have an ETL process that is running well but needs to be optimized further to reduce execution time.\n",
    "\n",
    "**Problem:** How would you set up performance monitoring and tuning in Informatica?\n",
    "\n",
    "**Response:**\n",
    "1. **Performance Metrics:**\n",
    "   - Define key performance indicators (KPIs) such as data load times, error rates, and resource utilization.\n",
    "     - Steps:\n",
    "       1. Identify critical metrics to monitor.\n",
    "       2. Set thresholds for acceptable performance levels.\n",
    "2. **Monitoring Tools:**\n",
    "   - Utilize Informatica’s built-in monitoring tools to track performance metrics in real-time.\n",
    "     - Steps:\n",
    "       1. Access the Informatica PowerCenter Monitor.\n",
    "       2. Configure dashboards to display relevant metrics.\n",
    "3. **Tuning Strategies:**\n",
    "   - Identify bottlenecks based on monitoring data and apply tuning strategies (e.g., parallelism, partitioning).\n",
    "     - Steps:\n",
    "       1. Analyze performance reports for slow operations.\n",
    "       2. Adjust configurations to optimize processing.\n",
    "4. **Performance Reports:**\n",
    "   - Generate regular reports to measure the effectiveness of performance improvements.\n",
    "     - Steps:\n",
    "       1. Schedule periodic report generation.\n",
    "       2. Review reports and make necessary adjustments.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=64,\n",
    ")\n",
    "\n",
    "document_65 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 8: Data Security and Compliance\n",
    "\n",
    "**Scenario:** Your company is implementing a new ETL solution that involves handling sensitive customer data. You need to ensure \n",
    "compliance with relevant regulations (e.g., GDPR).\n",
    "\n",
    "**Problem:** How would you address data security and compliance requirements in your ETL processes?\n",
    "\n",
    "**Response:**\n",
    "1. **Data Masking:**\n",
    "   - Implement data masking to protect sensitive information during development and testing.\n",
    "     - Steps:\n",
    "       1. Use Informatica’s Data Masking tool.\n",
    "       2. Define masking rules for sensitive fields.\n",
    "2. **Encryption:**\n",
    "   - Use encryption for secure data storage and transmission.\n",
    "     - Steps:\n",
    "       1. Encrypt data at rest using database features or file systems.\n",
    "       2. Implement SSL/TLS for data in transit.\n",
    "3. **Access Controls:**\n",
    "   - Define strict access controls based on user roles and responsibilities.\n",
    "     - Steps:\n",
    "       1. Set up role-based access control (RBAC).\n",
    "       2. Grant permissions only to authorized users.\n",
    "4. **Audit Trails:**\n",
    "   - Maintain audit trails for all data operations, including ETL processes.\n",
    "     - Steps:\n",
    "       1. Enable auditing in Informatica PowerCenter.\n",
    "       2. Store logs securely for compliance purposes.\n",
    "5. **Compliance Checks:**\n",
    "   - Regularly perform compliance checks and audits to ensure adherence to regulatory requirements.\n",
    "     - Steps:\n",
    "       1. Conduct regular internal audits.\n",
    "       2. Engage external auditors as needed.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=65,\n",
    ")\n",
    "\n",
    "document_66 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 9: Handling Large Volumes of Data\n",
    "\n",
    "**Scenario:** Your company is planning an ETL process that will handle a large volume of customer transaction data (e.g., billions of \n",
    "records).\n",
    "\n",
    "**Problem:** How would you approach handling such large volumes of data efficiently?\n",
    "\n",
    "**Response:**\n",
    "1. **Partitioning:**\n",
    "   - Partition data at the source system to reduce the amount of data processed during each run.\n",
    "     - Steps:\n",
    "       1. Identify suitable partition columns (e.g., date).\n",
    "       2. Configure the Extract stage in mappings to use partitioning.\n",
    "2. **Parallelism:**\n",
    "   - Increase parallel processing capabilities by adding more sessions or distributing data across multiple physical nodes.\n",
    "     - Steps:\n",
    "       1. Adjust the number of sessions in the workflow.\n",
    "       2. Use distributed processing if available.\n",
    "3. **Batch Processing:**\n",
    "   - Break the ETL process into smaller, manageable batches to avoid overwhelming the system.\n",
    "     - Steps:\n",
    "       1. Define batch sizes based on system capacity.\n",
    "       2. Schedule workflows to run in batches.\n",
    "4. **Resource Management:**\n",
    "   - Ensure that sufficient resources (e.g., memory, CPU) are allocated for handling large volumes of data.\n",
    "     - Steps:\n",
    "       1. Monitor resource usage during ETL runs.\n",
    "       2. Allocate additional resources as needed.\n",
    "5. **Performance Tuning:**\n",
    "   - Optimize mapping and workflow configurations to improve performance.\n",
    "     - Steps:\n",
    "       1. Analyze and optimize SQL queries.\n",
    "       2. Use Informatica’s tuning tools for further optimization.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=66,\n",
    ")\n",
    "\n",
    "document_67 = Document(\n",
    "    content=\"\"\"\n",
    "### Scenario 10: Implementing Data Quality Checks\n",
    "\n",
    "**Scenario:** You need to ensure the quality of the data being loaded into your ETL process, particularly in terms of completeness and \n",
    "accuracy.\n",
    "\n",
    "**Problem:** How would you implement robust data quality checks in your Informatica mappings?\n",
    "\n",
    "**Response:**\n",
    "1. **Data Profiling:**\n",
    "   - Use Informatica Data Quality tools to profile data and identify discrepancies or anomalies.\n",
    "     - Steps:\n",
    "       1. Open the Data Quality tool.\n",
    "       2. Load datasets for profiling.\n",
    "       3. Run profiling tasks to detect issues.\n",
    "2. **Validation Rules:**\n",
    "   - Define validation rules within ETL mappings to enforce business logic (e.g., date ranges, numeric constraints).\n",
    "     - Steps:\n",
    "       1. Use the Expression Editor or Validation components in mappings.\n",
    "       2. Implement checks for data integrity and consistency.\n",
    "3. **Error Handling:**\n",
    "   - Implement error handling mechanisms to capture and log invalid records.\n",
    "     - Steps:\n",
    "       1. Set up error handling components in mappings.\n",
    "       2. Configure logging to capture detailed error messages.\n",
    "4. **Data Cleansing:**\n",
    "   - Use Informatica’s data cleansing functions to correct or remove invalid data before loading.\n",
    "     - Steps:\n",
    "       1. Apply cleansing rules using the Cleansing component.\n",
    "       2. Validate and verify cleansed data.\n",
    "5. **Quality Reporting:**\n",
    "   - Generate reports summarizing the results of data quality checks for stakeholders.\n",
    "     - Steps:\n",
    "       1. Use Informatica’s reporting tools to create dashboards.\n",
    "       2. Schedule regular report generation.\n",
    "       3. Review reports and take corrective actions as needed.\n",
    "    \"\"\",\n",
    "    metadata={\"source\": \"Informatica\"},\n",
    "    id=67,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bb612d-e18c-47eb-a03b-cb3e39476086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = [document_1, document_2, document_3, document_4, document_5, document_6, document_7, document_8, document_9, document_10, document_11, document_12, document_13, document_14, document_15, document_16, document_17, document_18, document_19, document_20, document_21, document_22, document_23, document_24, document_25, document_26, document_27, document_28, document_29, document_30, document_31, document_32, document_33, document_34, document_35, document_36, document_37, document_38, document_39, document_40, document_41, document_42, document_43, document_44, document_45, document_46, \n",
    "             document_47, document_48, document_49, document_50, \n",
    "             document_51, document_52, document_53, document_54, document_55, document_56, document_57, document_58, document_59, document_60, document_61, document_62, document_63, document_64, document_65, document_66, document_67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfd774a-8923-49ec-ad16-efe709b3b4ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vectors shape: (67, 384)\n",
      "Query vector shape: (1, 384)\n",
      "\n",
      "### Scenario 3: Troubleshoot Problems in Production\n",
      "\n",
      "**Scenario:** You are monitoring your ETL processes and notice that the customer transaction load is significantly delayed. The logs \n",
      "indicate errors related to data quality issues.\n",
      "\n",
      "**Problem:** What steps would you take to diagnose and resolve the issue?\n",
      "\n",
      "**Response:**\n",
      "1. **Review Logs:**\n",
      "   - Analyze recent logs for specific error messages indicating data quality issues.\n",
      "     - Steps:\n",
      "       1. Access Informatica PowerCenter’s log files.\n",
      "       2. Search for error codes and messages related to customer transactions.\n",
      "2. **Data Profiling:**\n",
      "   - Use Informatica Data Quality tools to profile customer and transaction data, identifying discrepancies or anomalies.\n",
      "     - Steps:\n",
      "       1. Open Informatica Data Quality tool.\n",
      "       2. Load the relevant datasets (customer details, transactions).\n",
      "       3. Run profiling tasks to identify issues like missing values, duplicates, etc.\n",
      "3. **Re-run with Validation:**\n",
      "   - Rerun the ETL process with enhanced validation logic to filter out invalid records.\n",
      "     - Steps:\n",
      "       1. Modify mappings to include additional validation rules.\n",
      "       2. Execute the workflow again and monitor for errors.\n",
      "4. **Correct Source Data:**\n",
      "   - If the issue is due to incorrect source data, work with the source systems team to correct or update the data.\n",
      "     - Steps:\n",
      "       1. Identify the root cause of the data quality issues.\n",
      "       2. Communicate with the source system team for corrections.\n",
      "5. **Improve Error Handling:**\n",
      "   - Enhance error handling in mappings and workflows to prevent similar issues in future runs.\n",
      "     - Steps:\n",
      "       1. Implement more robust error handling mechanisms.\n",
      "       2. Configure notifications for recurring errors.\n",
      "    \n",
      "Question: What is ETL in the context of Informatica Cloud Data Integration?\n",
      "Answer: In Informatica Cloud, ETL stands for Extract, Transform, and Load. It refers to the process of extracting data from source \n",
      "systems, transforming it into a desired format, and loading it into target systems or data warehouses. Informatica Cloud provides tools \n",
      "and services to automate this process, ensuring data accuracy and consistency across different environments.\n",
      "\n",
      "Scenario: You need to ensure the quality of the data being loaded into your ETL process, particularly in terms of completeness and \n",
      "accuracy.\n",
      "Problem: How would you implement robust data quality checks in your Informatica mappings?\n",
      "Response:\n",
      "1. **Data Profiling:** Use Informatica’s Data Quality tools to profile data and identify missing values, duplicates, and anomalies.\n",
      "2. **Validation Rules:** Define validation rules within ETL mappings to enforce business logic (e.g., date ranges, numeric constraints).\n",
      "3. **Error Handling:** Implement error handling mechanisms to capture and log invalid records for further review.\n",
      "4. **Data Cleansing:** Use Informatica’s data cleansing functions to correct or remove invalid data before loading.\n",
      "5. **Quality Reporting:** Generate reports summarizing the results of data quality checks for stakeholders.\n",
      "Question: Explain ETL concepts in detail with respect to Informatica Cloud.\n",
      "Answer: ETL is the process of extracting data from various sources, transforming it into a usable format, and loading it into a data \n",
      "warehouse or another target system. In Informatica Cloud, this involves several key steps:\n",
      "1. **Extract**: Gathering data from sources using extractors.\n",
      "2. **Transform**: Cleaning, enriching, and formatting data to meet business requirements using transformation mappings.\n",
      "3. **Load**: Storing the transformed data in the target systems using loaders. Informatica Cloud supports a wide range of connectors for \n",
      "seamless integration with different data sources and targets.\n",
      "\n",
      "### Scenario 10: Implementing Data Quality Checks\n",
      "\n",
      "**Scenario:** You need to ensure the quality of the data being loaded into your ETL process, particularly in terms of completeness and \n",
      "accuracy.\n",
      "\n",
      "**Problem:** How would you implement robust data quality checks in your Informatica mappings?\n",
      "\n",
      "**Response:**\n",
      "1. **Data Profiling:**\n",
      "   - Use Informatica Data Quality tools to profile data and identify discrepancies or anomalies.\n",
      "     - Steps:\n",
      "       1. Open the Data Quality tool.\n",
      "       2. Load datasets for profiling.\n",
      "       3. Run profiling tasks to detect issues.\n",
      "2. **Validation Rules:**\n",
      "   - Define validation rules within ETL mappings to enforce business logic (e.g., date ranges, numeric constraints).\n",
      "     - Steps:\n",
      "       1. Use the Expression Editor or Validation components in mappings.\n",
      "       2. Implement checks for data integrity and consistency.\n",
      "3. **Error Handling:**\n",
      "   - Implement error handling mechanisms to capture and log invalid records.\n",
      "     - Steps:\n",
      "       1. Set up error handling components in mappings.\n",
      "       2. Configure logging to capture detailed error messages.\n",
      "4. **Data Cleansing:**\n",
      "   - Use Informatica’s data cleansing functions to correct or remove invalid data before loading.\n",
      "     - Steps:\n",
      "       1. Apply cleansing rules using the Cleansing component.\n",
      "       2. Validate and verify cleansed data.\n",
      "5. **Quality Reporting:**\n",
      "   - Generate reports summarizing the results of data quality checks for stakeholders.\n",
      "     - Steps:\n",
      "       1. Use Informatica’s reporting tools to create dashboards.\n",
      "       2. Schedule regular report generation.\n",
      "       3. Review reports and take corrective actions as needed.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "semantic_search = SemanticSearch(documents)\n",
    "semantic_search.load_documents()\n",
    "\n",
    "query = \"Troubleshooting a ETL Job\"\n",
    "top_5_docs = semantic_search.search(query, top_k=5)\n",
    "\n",
    "for doc in top_5_docs:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
