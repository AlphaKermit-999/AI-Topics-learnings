{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnIWSWCNaOgc"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to implement word embeddings - Semantic meaning\n",
        "\n",
        "Solution\n",
        "--\n",
        "Word embeddings are prediction based, and they use shallow neural networks to train the model that will lead to learning the weight and using them as a vector representation.\n",
        "\n",
        "<font color='green'>word2vec</font>\n",
        "--\n",
        "**word2vec** is the deep learning Google framework to train word embeddings. It will use all the words of the whole corpus and predict\n",
        "the nearby words. It will create a vector for all the words present in the\n",
        "corpus in a way so that the context is captured. It also outperforms any\n",
        "other methodologies in the space of word similarity and word analogies.\n",
        "\n",
        "There are mainly 2 types of word2vec Model.\n",
        "\n",
        "• Skip-Gram\n",
        "\n",
        "• Continuous Bag of Words (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqmDRw1RaOgc"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ZC7kOYkuY2BGRCONWde38usTOCRJqJlR\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsJ6785laOgd"
      },
      "source": [
        "The above figure shows the architecture of the CBOW and skip-gram\n",
        "algorithms used to build word embeddings. Let us see how these models\n",
        "work in detail.\n",
        "\n",
        "Skip-Gram\n",
        "--\n",
        "The skip-gram model is used to predict the probabilities of a word given the context of word or words.\n",
        "\n",
        "Let us take a small sentence and understand how it actually works.\n",
        "Each sentence will generate a target word and context, which are the words\n",
        "nearby. The number of words to be considered around the target variable\n",
        "is called the window size. The table below shows all the possible target\n",
        "and context variables for window size 2. Window size needs to be selected\n",
        "based on data and the resources at your disposal. The larger the window\n",
        "size, the higher the computing power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwTt4-BMaOge"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=18nKDL_JAX96Zs_ILGMrcdd517GWLwrW2\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtDnwZltaOgf"
      },
      "source": [
        "Since it takes a lot of text and computing power, let us go ahead and take sample data and build a skip-gram model.\n",
        "\n",
        "As mentioned *in earlier NB's*, import the text corpus and break it into sentences. Perform **some cleaning and preprocessing** like the removal of\n",
        "punctuation and digits, and split the sentences into words or tokens, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "KHgDn54KMFQa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "0c3d4cd3-1fd8-4e69-8b38-e722dd3657b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f240d28db04744379f03c02f8276eb96"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f5sRLVEOaOgf"
      },
      "outputs": [],
      "source": [
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves','lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]\n",
        "\n",
        "#import library\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from matplotlib import pyplot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO-CS4IBD5GK",
        "outputId": "0c591359-ec92-4797-a925-a6bef709374c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "7\n",
            "3\n",
            "9\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "length=[]\n",
        "for i in sentences:\n",
        "    print(len(i))\n",
        "    length.append(len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eqMxH-JbD5GL"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcpzDYUyD5GL",
        "outputId": "f063aeeb-a1b2-4fc4-c4e8-dbd2f9555eea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "np.mean(length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiCvHqkYD5GL",
        "outputId": "686051cf-b854-4d0d-98dd-2d263a5187be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "np.median(length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki7kiRq7D5GM",
        "outputId": "da7d06fe-c09a-4505-b9b3-c8054152aa42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "np.min(length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS5H4J8LD5GM",
        "outputId": "f67af7f7-914b-410d-b1df-82f171937bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "np.max(length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-tvJasD5GM"
      },
      "source": [
        "### training the model\n",
        "https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabory = 100\n",
        "# vector size=2, king = [0.1, 0.2], xy=[0.2,0.3]"
      ],
      "metadata": {
        "id": "7cdCbVP6MvuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CzBCPYZ3aOgj"
      },
      "outputs": [],
      "source": [
        "\n",
        "skipgram = Word2Vec(sentences, vector_size = 50, window = 3, min_count=1,sg = 0,epochs=5)\n",
        "# vector_size : int, optional\n",
        "#     Dimensionality of the word vectors.\n",
        "# window : int, optional\n",
        "#     Maximum distance between the current and predicted word within a sentence.\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant. (default 5)\n",
        "# workers -> How many threads to use behind the scenes? (default 3)\n",
        "# sg -> (default 0 or CBOW) The training algorithm, either CBOW (0)\n",
        "#                           or skip gram (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9FC4NeRgD5GM"
      },
      "outputs": [],
      "source": [
        "# distribution\n",
        "# Word2Vec?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PComtlWpD5GM",
        "outputId": "e05318ca-409a-4861-919a-13bb770d1a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nlp': 0, 'I': 1, 'future': 2, 'love': 3, 'will': 4, 'learn': 5, 'in': 6, '2': 7, 'months': 8, 'is': 9, 'learning': 10, 'machine': 11, 'time': 12, 'and': 13, 'solves': 14, 'lot': 15, 'of': 16, 'industry': 17, 'problems': 18, 'uses': 19, 'saves': 20}\n"
          ]
        }
      ],
      "source": [
        "print(skipgram.wv.key_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnm5Mb0YD5GN"
      },
      "source": [
        "### access vector for one word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1gBLG9D5GN",
        "outputId": "bbc09e16-7c9c-489a-e689-9ef0c6e0bc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(skipgram.wv['nlp']))  # get numpy vector of a word\n",
        "\n",
        "# Since our vector size parameter was 50, the model\n",
        "# gives a vector of size 50 for each word."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"i didn't like the product\":50\n",
        "# [..]+[..],[...][...][....]/5=50"
      ],
      "metadata": {
        "id": "JZf_ptjOTAjd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BaJM6D4D5GN"
      },
      "source": [
        "### Similar to a word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtHP3HwbD5GN",
        "outputId": "3870073c-3c66-4647-cb74-fb551637f22f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('machine', 0.5294367074966431), ('nlp', 0.21074169874191284)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "skipgram.wv.most_similar('solves', topn=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZzXvIeAD5GN"
      },
      "source": [
        "### Saving a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ly-tTsKkD5GN"
      },
      "outputs": [],
      "source": [
        "skipgram.save(\"skipgram.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PQwsSewD5GN"
      },
      "source": [
        "### Reloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt5aw68vD5GN",
        "outputId": "98e260ab-2764-462f-810e-462e64491195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('and', 0.3066261410713196), ('industry', 0.27047690749168396)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model = Word2Vec.load(\"skipgram.model\")\n",
        "model.wv.most_similar('learn', topn=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj0hhQnxD5GN",
        "outputId": "0daa47e1-dff5-4268-909d-ca4047ec08c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('learn', 0.27047690749168396), ('in', 0.2048206925392151)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model.wv.most_similar('industry', topn=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg7AOHVdD5GN",
        "outputId": "58a93246-46b7-4b03-d7da-4c3e3d7e591e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('solves', 0.5294366478919983), ('of', 0.2258927822113037)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.wv.most_similar('machine', topn=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeXd2R78D5GO"
      },
      "source": [
        "#### Training an actual corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ygFvEkLD5GO"
      },
      "source": [
        "#### TASK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G80U23DUD5GO"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Human machine interface, for lab abc& computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection graph of paths in trees?\",\n",
        "    \"Graph minors IV Widths of trees and well. quasi ordering\",\n",
        "    \"Graph minors A survey\",\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOtm39qGD5GO"
      },
      "source": [
        "### Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YHdCitUrD5GO"
      },
      "outputs": [],
      "source": [
        "# cleaning the texts\n",
        "\n",
        "# remove common words and tokenize\n",
        "\n",
        "# remove words that appear only once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuI-1Ik_D5GO"
      },
      "source": [
        "### Activity- Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V_W0dHM4D5GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e5b5fa-d82f-47de-c2f4-d94f1861f691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['human', 'interface', 'computer'],\n",
            " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'system'],\n",
            " ['system', 'human', 'system', 'eps'],\n",
            " ['user', 'response', 'time'],\n",
            " ['trees'],\n",
            " ['graph', 'trees'],\n",
            " ['graph', 'minors', 'trees'],\n",
            " ['graph', 'minors', 'survey']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:20: SyntaxWarning: invalid escape sequence '\\?'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\.'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\?'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\.'\n",
            "/tmp/ipython-input-3387274789.py:20: SyntaxWarning: invalid escape sequence '\\?'\n",
            "  sent=re.sub(\"\\?\",\"\",sent)\n",
            "/tmp/ipython-input-3387274789.py:21: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  sent=re.sub(\"\\.\",\"\",sent)\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint  # pretty-printer\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "#Get the character set\n",
        "characters=set()\n",
        "for sent in documents:\n",
        "    for word in sent.split():\n",
        "        for char in word:\n",
        "            characters.add(char.lower())\n",
        "\n",
        "# cleaning the texts\n",
        "\n",
        "documents_clean=[]\n",
        "\n",
        "for sent in documents:\n",
        "#     print(sent)\n",
        "    sent=re.sub(\"&\",\"\",sent)\n",
        "    sent=re.sub(\",\",\"\",sent)\n",
        "    sent=re.sub(\"\\?\",\"\",sent)\n",
        "    sent=re.sub(\"\\.\",\"\",sent)\n",
        "#     print(sent)\n",
        "    documents_clean.append(sent)\n",
        "\n",
        "# remove common words and tokenize\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "texts = [\n",
        "    [word for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents_clean\n",
        "]\n",
        "\n",
        "# remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [\n",
        "    [token for token in text if frequency[token] > 1]\n",
        "    for text in texts\n",
        "]\n",
        "\n",
        "pprint(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H62Mmb3cD5GO"
      },
      "outputs": [],
      "source": [
        "skipgram = Word2Vec(texts, vector_size = 50, window = 3, min_count=1,sg = 1,epochs=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZqCCcBODD5GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb2801b-9539-4fde-e1c3-0b91b9ebf365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'system': 0, 'graph': 1, 'trees': 2, 'user': 3, 'minors': 4, 'eps': 5, 'time': 6, 'response': 7, 'survey': 8, 'computer': 9, 'interface': 10, 'human': 11}\n"
          ]
        }
      ],
      "source": [
        "print(skipgram.wv.key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7FiWjBkyD5GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c59abea-e253-4fef-d4ea-5dbe6f2a7efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eps', 0.22442299127578735), ('system', 0.0998455286026001), ('time', 0.089928537607193), ('human', 0.058373644948005676), ('graph', 0.0013571253512054682), ('response', -0.0013637219090014696), ('trees', -0.037274789065122604), ('minors', -0.06371434032917023), ('interface', -0.11219383776187897), ('user', -0.12241575121879578)]\n"
          ]
        }
      ],
      "source": [
        "vector = skipgram.wv['computer']  # get numpy vector of a word\n",
        "sims = skipgram.wv.most_similar('computer', topn=10)  # get other similar words\n",
        "print(sims)\n",
        "#Try with more number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iA8GgYbUD5GO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "4a1167a4-39bd-4af3-bcec-f44d23908e24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'aayush' not present\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1733930052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mskipgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aayush'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'aayush' not present\""
          ]
        }
      ],
      "source": [
        " skipgram.wv['aayush']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrTBufNtaOgs"
      },
      "source": [
        "**Note** : We get an error saying the word doesn’t exist because this word was not there in our input training data. This is the reason we need to train the algorithm on as much data possible so that we do not miss out on words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FICEBdluaOgs"
      },
      "source": [
        "Continuous Bag of Words (CBOW)\n",
        "--\n",
        "Now let’s see how to build CBOW model. (Its very similar to SkipGram model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0Lz6bOpqaOgt"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YBRX5kEIaOgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0eefb7-e1f2-4c60-e18b-8dbefca126a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.1892752e-04  1.8471200e-04  3.9869919e-03  7.0384946e-03\n",
            " -7.2679296e-03 -5.5600069e-03  5.0459942e-03  7.0101470e-03\n",
            " -3.9183032e-03 -2.9401341e-03  5.7660192e-03 -1.1980245e-03\n",
            " -3.5442291e-03  5.1203528e-03 -3.7970003e-03 -1.4187638e-03\n",
            "  2.2473279e-03  7.7490136e-04 -6.4728241e-03 -7.3818890e-03\n",
            "  5.7123173e-03  3.9611422e-03  5.2794479e-03  5.9598871e-04\n",
            "  4.9616331e-03 -2.6604421e-03 -7.3937606e-04  4.5066979e-03\n",
            " -5.8762794e-03 -3.0750809e-03 -5.8684237e-03 -7.2659552e-04\n",
            "  7.4516553e-03 -5.7180990e-03 -1.8232567e-03 -1.5138602e-03\n",
            "  6.3104974e-03 -4.6335123e-03  3.5283156e-05 -3.7138546e-03\n",
            " -7.5027738e-03  3.9119478e-03 -6.8434263e-03 -3.4311134e-03\n",
            " -2.7421862e-05 -2.3139175e-04 -5.9853438e-03  7.5115180e-03\n",
            "  3.8922327e-03  7.2133932e-03 -6.3733729e-03  3.5123425e-03\n",
            " -3.2320907e-03  6.4416882e-04  6.6395467e-03 -3.4860754e-03\n",
            "  3.5292972e-03 -5.3023128e-03 -2.7722567e-03  7.3425844e-03\n",
            " -1.2325412e-03  2.5107153e-04 -3.2348670e-03 -6.0021002e-03\n",
            " -1.1781314e-03  1.9295271e-03 -6.9377106e-04  4.3231733e-03\n",
            " -2.1429509e-03  1.7656758e-03  4.2623393e-03  6.5202760e-03\n",
            " -1.1357348e-03 -7.1938615e-03  3.4144940e-03  4.4670701e-04\n",
            "  5.8139907e-03 -6.3537713e-04 -2.0612609e-03 -6.8382882e-03\n",
            " -6.6918507e-04  2.2082524e-03  4.2198664e-03  5.5098878e-03\n",
            " -4.4555636e-03  1.4522029e-03  4.7569247e-03 -3.7484774e-03\n",
            " -2.4275472e-03  5.3106481e-03  1.2745904e-03  1.4837272e-04\n",
            "  2.7137790e-03  1.7013866e-04  7.5147077e-03  3.9535966e-03\n",
            " -6.9667110e-03 -5.5012191e-03  7.0426241e-04  4.9941670e-03\n",
            " -6.7341309e-03  2.8638579e-03  4.0545966e-03  4.4858893e-03\n",
            "  5.8335299e-03 -4.8184963e-03  8.6376071e-04  4.7244392e-03\n",
            " -2.2187894e-03 -4.8230644e-03 -3.2048672e-04 -6.5382412e-03\n",
            " -4.3750098e-03  5.5504208e-03  2.6191715e-03  5.6450544e-03\n",
            "  5.3126933e-03  5.8833919e-03 -2.9602768e-03 -4.3891091e-04\n",
            "  1.8346692e-03 -3.5304939e-03  6.5536965e-03 -7.7016903e-03\n",
            "  5.2848756e-03  2.2768881e-03 -3.8537746e-03  3.4360839e-03]\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "cbow = Word2Vec(sentences, vector_size =128, window = 3, min_count=1,sg = 0)\n",
        "# size=50 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# as sg=0 i.e no skipgram , hence default CBOW\n",
        "\n",
        "# access vector for one word\n",
        "print(cbow.wv['nlp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egvbeVOaOg1"
      },
      "source": [
        "Important Observation\n",
        "--\n",
        "To train these models, it requires a huge amount of computing\n",
        "power. So, let us go ahead and use Google’s pre-trained model, which has\n",
        "been trained with over 100 billion words.\n",
        "\n",
        "Download the model from the below path and keep it in your local\n",
        "storage:\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "\n",
        "or **better off from this link** :\n",
        "\n",
        "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "Note **if running on Jupyter NB** : The Google Db is soo large that we would get ValueError, like this : ValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DtdVkgeHaOg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "755ce091-1419-4988-fb48-cb2b3eb25f52"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "404 Client Error: Not Found for url: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3138653788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# model = gensim.models.KeyedVectors.load_word2vec_format('~/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     filename = (\n\u001b[1;32m    219\u001b[0m         \u001b[0mbinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smart_open/http.py\u001b[0m in \u001b[0;36mopen_uri\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smart_open/http.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, kerberos, user, password, cert, headers, timeout, session, buffer_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_BINARY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         fobj = SeekableBufferedInputBase(\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerberos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkerberos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/smart_open/http.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, mode, buffer_size, kerberos, user, password, cert, headers, session, timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'self.response: %r, raw: %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# import gensim package\n",
        "import gensim\n",
        "\n",
        "# load the saved model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('~/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Jifu4g93aOg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "67ee975a-6962-4e7c-f632-d0cc456ec974"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2Vec' object has no attribute 'similarity'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2599674617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lets check similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Lets check one more.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'book'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'similarity'"
          ]
        }
      ],
      "source": [
        "# lets check similarity\n",
        "print (model.similarity('This', 'is'))\n",
        "\n",
        "#Lets check one more.\n",
        "print (model.similarity('post', 'book'))\n",
        "\n",
        "#print(model.similarity('seed', 'need'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS5_M8NZaOg9"
      },
      "source": [
        "“`This`” and “`is`” have a good amount of similarity, but the similarity\n",
        "between the words “`post`” and “`book`” is poor. For any given set of words, it uses the vectors of both the words and calculates the similarity between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TT3ITyB4aOg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "598867d6-02fd-4619-b41c-5d47c9867508"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2Vec' object has no attribute 'doesnt_match'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2849995381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Finding the odd one out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'breakfast cereal dinner lunch'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'doesnt_match'"
          ]
        }
      ],
      "source": [
        "# Finding the odd one out.\n",
        "model.doesnt_match('breakfast cereal dinner lunch'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r6RHk1laOhB"
      },
      "source": [
        "Of '`breakfast`’, ‘`cereal`’, ‘`dinner`’ and ‘`lunch`', only **cereal** is the word that is\n",
        "not anywhere related to the remaining 3 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tbbBJX3kaOhC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "df44015b-171b-4688-f5b6-ceca924a164a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2Vec' object has no attribute 'most_similar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-694780633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# try this too :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'most_similar'"
          ]
        }
      ],
      "source": [
        "# It is also finding the relations between words.\n",
        "#model.most_similar(positive=['woman', 'king'] , negative=['man'])  # default value of topn is 10\n",
        "\n",
        "# try this too :\n",
        "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRLzCzYOaOhH"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=11Yu1Gj4Rw5BccL6KXnT_rXqYPyJbEUfZ\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76A5vL7SD5GU"
      },
      "source": [
        "![Screen%20Shot%202021-04-10%20at%202.37.02%20AM.png](attachment:Screen%20Shot%202021-04-10%20at%202.37.02%20AM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "x3uIxa4PD5GV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2890e009-3682-40cd-e77a-88c949e7b3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300',\n",
            " 'conceptnet-numberbatch-17-06-300',\n",
            " 'word2vec-ruscorpora-300',\n",
            " 'word2vec-google-news-300',\n",
            " 'glove-wiki-gigaword-50',\n",
            " 'glove-wiki-gigaword-100',\n",
            " 'glove-wiki-gigaword-200',\n",
            " 'glove-wiki-gigaword-300',\n",
            " 'glove-twitter-25',\n",
            " 'glove-twitter-50',\n",
            " 'glove-twitter-100',\n",
            " 'glove-twitter-200',\n",
            " '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# Show all available models in gensim-data\n",
        "\n",
        "pprint(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OGTs4HW7D5GV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594ce1f6-e96d-4fb3-efbe-80cfd47db56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.948005199432373),\n",
              " ('tweet', 0.9403423070907593),\n",
              " ('fb', 0.9342358708381653),\n",
              " ('instagram', 0.9104824066162109),\n",
              " ('chat', 0.8964964747428894),\n",
              " ('hashtag', 0.8885937333106995),\n",
              " ('tweets', 0.8878158330917358),\n",
              " ('tl', 0.8778461217880249),\n",
              " ('link', 0.8778210878372192),\n",
              " ('internet', 0.8753897547721863)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Download the \"glove-twitter-25\" embeddings\n",
        "\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "# Use the downloaded vectors as usual:\n",
        "\n",
        "glove_vectors.most_similar('twitter')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CUzPWWjapy6",
        "outputId": "53cdcf35-94a3-4c3c-d72f-27276619ea2d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('meets', 0.8841924071311951),\n",
              " ('prince', 0.832163393497467),\n",
              " ('queen', 0.8257461190223694)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT1rvQhRaOhI"
      },
      "source": [
        "Implementing <font color='green'>fastText</font>\n",
        "--\n",
        "**fastText** is another deep learning framework developed by Facebook to capture context and meaning.\n",
        "\n",
        "Problem\n",
        "--\n",
        "How to implement fastText in Python.\n",
        "\n",
        "Solution\n",
        "--\n",
        "fastText is the improvised version of word2vec. word2vec basically\n",
        "considers words to build the representation. But fastText takes each\n",
        "character while computing the representation of the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YNtIJNqnaOhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f932b323-2cc8-4fca-dfc9-418547a4e1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.5651671e-03  9.3698065e-04 -2.0309705e-03  1.3372294e-03\n",
            "  1.0669909e-03 -1.3030644e-03  4.5768594e-04 -3.4850935e-04\n",
            "  3.0172872e-04 -3.3416669e-04  2.3032913e-03 -2.1352421e-03\n",
            " -3.5649028e-03 -2.0423550e-03  1.6025617e-04  3.4330557e-03\n",
            "  1.6206586e-04 -1.7705902e-03  2.4007181e-04 -2.9109141e-03\n",
            "  4.3194192e-03 -2.1401787e-04  9.1965008e-04 -1.9673463e-03\n",
            "  4.2877963e-04  1.9754227e-03 -5.5509841e-04 -7.4600609e-04\n",
            " -1.6250368e-04  4.6096946e-04 -4.1272063e-03 -3.7344757e-03\n",
            "  7.8168191e-04  6.3480536e-04 -2.7865777e-03  1.0397271e-03\n",
            "  7.2312576e-04  4.0080363e-04 -1.0363614e-03  6.7208544e-04\n",
            "  5.9388904e-04  1.4350816e-03 -9.1749663e-04  1.1620179e-03\n",
            " -4.6320874e-03 -7.4696593e-04 -1.8302952e-03 -1.8878000e-04\n",
            "  5.4830208e-04 -1.0331636e-03 -9.7611046e-04 -6.6045811e-04\n",
            " -1.0406146e-03  4.3693674e-03  3.6964498e-03 -2.3840894e-03\n",
            "  1.0004241e-03  1.6795534e-04 -4.5526810e-03  8.8466250e-04\n",
            "  2.2786174e-03 -1.2467529e-04 -2.6847786e-04  2.6513743e-03\n",
            " -1.5358579e-03 -4.6134781e-04  1.1842061e-04  3.4597358e-03\n",
            "  3.7217830e-04  2.2008002e-03 -1.2545080e-03  1.3967918e-03\n",
            "  2.8876966e-05 -3.9235512e-03  3.2909494e-04 -6.1132538e-05\n",
            "  2.9919816e-03 -1.4361407e-03 -1.9196585e-03 -1.6801087e-03\n",
            " -1.4594588e-03 -2.0364631e-04  8.6144428e-04 -2.1820571e-03\n",
            " -1.7899858e-03  1.0026211e-03  2.6793042e-03  5.6621025e-04\n",
            " -6.3928036e-04  1.2664252e-03  2.1741202e-04  2.7880999e-03\n",
            "  3.4232331e-03  2.9016552e-03  2.5485216e-03  8.7576023e-05\n",
            " -2.5145016e-03 -2.6166893e-03  2.4390395e-04  5.0592521e-04]\n"
          ]
        }
      ],
      "source": [
        "# Let us see how to build a fastText word embedding.\n",
        "# Import FastText\n",
        "from gensim.models import FastText\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Example sentences\n",
        "sentences = [['I', 'love', 'nlp'],\n",
        "['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
        "['nlp', 'is', 'future'],\n",
        "[ 'nlp', 'saves', 'time', 'and', 'solves',\n",
        "'lot', 'of', 'industry', 'problems'],\n",
        "['nlp', 'uses', 'machine', 'learning']]\n",
        "\n",
        "fast = FastText(sentences,window=1, min_count=1, workers=5, min_n=1, max_n=2)\n",
        "# size=10 -> means size of vector to represent each token or word\n",
        "# window=1 -> The maximum distance between the target word and its neighboring word.\n",
        "# min_count=1 -> Minimium frequency count of words.\n",
        "#                The model would ignore words that do not satisfy the min_count.\n",
        "#                Extremely infrequent words are usually unimportant.\n",
        "# workers -> How many threads to use behind the scenes?\n",
        "# min_n=1, max_n=2  -> When finding similarity or analogies like this :\n",
        "# \"Father\" - \"Boy\" + \"Girl\" == \"Mother\"\n",
        "#print(fast.most_similar(['girl', 'father'], ['boy'], topn=3))\n",
        "# [('mother', 0.7996115684509277), ('grandfather', 0.7629683613777161),\n",
        "# ('wife', 0.7478234767913818)]\n",
        "# we want the model to show min 1 and max 2 analogies\n",
        "\n",
        "\n",
        "# vector for word nlp\n",
        "print(fast.wv['nlp'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dyPmRNYn_9tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7e7f1c-de5b-4e2c-c5cd-70b62b9fab02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('learn', 0.352792888879776), ('love', 0.23435966670513153), ('industry', 0.16918529570102692)]\n"
          ]
        }
      ],
      "source": [
        "# Try this\n",
        "print(fast.wv.most_similar(['machine', 'learning'], ['nlp'], topn=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88V9XH1EBHpV"
      },
      "source": [
        "\n",
        "<hr>\n",
        "<br><br>\n",
        "<u><b>Further Resources</b></u> :\n",
        "\n",
        "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
        "\n",
        "https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}